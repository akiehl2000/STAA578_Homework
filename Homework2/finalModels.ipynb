{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2 Final Models (Loan Prediction)\n",
    "\n",
    "Adam Kiehl  \n",
    "4/23/23"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/akiehl/miniconda3/envs/dsci/lib/python3.8/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "/Users/akiehl/miniconda3/envs/dsci/lib/python3.8/site-packages/tensorflow_addons/utils/ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.10.0 and strictly below 2.13.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.9.2 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# import analysis packages\n",
    "import keras\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras import models\n",
    "from keras.regularizers import l2\n",
    "from keras.utils import to_categorical\n",
    "import keras.backend as back\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.compose import make_column_selector\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow_addons.metrics import F1Score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data from .csvs\n",
    "trainDF = pd.read_csv('./loan_train.csv')\n",
    "testDF = pd.read_csv('./loan_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate response/prediction columns\n",
    "trainResp = np.where(trainDF['MIS_Status'] == 'P I F', 1, 0)\n",
    "trainDF.drop('MIS_Status', axis = 1, inplace = True)\n",
    "testIDs = testDF['CustomerId']\n",
    "testDF.drop('CustomerId', axis = 1, inplace = True)\n",
    "\n",
    "# combine data sets for preprocessing\n",
    "trainDF['source'] = 'train'\n",
    "testDF['source'] = 'test'\n",
    "fullDF = pd.concat([trainDF, testDF], axis = 0)\n",
    "\n",
    "# factor categorical predictors\n",
    "fullDF['NAICS'] = fullDF['NAICS'].apply(lambda x: str(x))\n",
    "fullDF['NewExist'] = fullDF['NewExist'].apply(lambda x: str(x))\n",
    "fullDF['UrbanRural'] = fullDF['UrbanRural'].apply(lambda x: str(x))\n",
    "fullDF['RevLineCr'] = np.where(fullDF['RevLineCr'] == 'Y', 'Y', 'N')\n",
    "fullDF['LowDoc'] = np.where(fullDF['LowDoc'] == 'Y', 'Y', 'N')\n",
    "fullDF['New'] = fullDF['New'].apply(lambda x: str(x))\n",
    "fullDF['RealEstate'] = fullDF['RealEstate'].apply(lambda x: str(x))\n",
    "fullDF['Recession'] = fullDF['Recession'].apply(lambda x: str(x))\n",
    "\n",
    "# selected predictors\n",
    "predictors = ['NAICS', 'Term', 'NoEmp', 'CreateJob', 'RetainedJob', 'UrbanRural', 'RevLineCr', 'LowDoc', 'DisbursementGross', 'GrAppv', 'New', 'RealEstate', 'Portion', 'Recession']\n",
    "src = fullDF['source']\n",
    "fullDF = fullDF[predictors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale numeric predictors and encode categorical predictors\n",
    "findNumPredictors = make_column_selector(dtype_exclude = object)\n",
    "findCatPredictors = make_column_selector(dtype_include = object)\n",
    "transform = make_column_transformer((MinMaxScaler(), findNumPredictors),\n",
    "                                    (OneHotEncoder(), findCatPredictors))\n",
    "\n",
    "# get new column names\n",
    "colNames = transform.fit(fullDF).get_feature_names_out()\n",
    "\n",
    "# transform data\n",
    "modelDF = pd.DataFrame(transform.fit_transform(fullDF), columns = colNames)\n",
    "\n",
    "# split data into training, validation, and test sets\n",
    "modelTrain = modelDF.loc[np.where(src == 'train')]\n",
    "modelTest = modelDF.iloc[np.where(src == 'test')]\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(modelTrain, trainResp, test_size = 0.2, random_state = 4192023)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosted Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuned hyperparameters\n",
    "L = 0.1\n",
    "D = 1\n",
    "\n",
    "# fit gradient boosted model\n",
    "model2 = GradientBoostingClassifier(learning_rate = L, max_depth = D, n_estimators = 1000, random_state = 4192023)\n",
    "model2.fit(X_train, y_train)\n",
    "\n",
    "# predict on validation set\n",
    "pred2 = model2.predict(X_valid)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_6 (Dense)             (None, 512)               23040     \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 197,633\n",
      "Trainable params: 197,633\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "14/14 [==============================] - 1s 33ms/step - loss: 0.6342 - accuracy: 0.6061 - f1_score: 0.6916\n",
      "Epoch 2/50\n",
      "14/14 [==============================] - 0s 15ms/step - loss: 0.6046 - accuracy: 0.6652 - f1_score: 0.7230\n",
      "Epoch 3/50\n",
      "14/14 [==============================] - 0s 15ms/step - loss: 0.5979 - accuracy: 0.6720 - f1_score: 0.7250\n",
      "Epoch 4/50\n",
      "14/14 [==============================] - 0s 14ms/step - loss: 0.5950 - accuracy: 0.6799 - f1_score: 0.7304\n",
      "Epoch 5/50\n",
      "14/14 [==============================] - 0s 16ms/step - loss: 0.5886 - accuracy: 0.6890 - f1_score: 0.7420\n",
      "Epoch 6/50\n",
      "14/14 [==============================] - 0s 15ms/step - loss: 0.5751 - accuracy: 0.7026 - f1_score: 0.7490\n",
      "Epoch 7/50\n",
      "14/14 [==============================] - 0s 15ms/step - loss: 0.5665 - accuracy: 0.7060 - f1_score: 0.7582\n",
      "Epoch 8/50\n",
      "14/14 [==============================] - 0s 14ms/step - loss: 0.5677 - accuracy: 0.6981 - f1_score: 0.7514\n",
      "Epoch 9/50\n",
      "14/14 [==============================] - 0s 14ms/step - loss: 0.5597 - accuracy: 0.7117 - f1_score: 0.7586\n",
      "Epoch 10/50\n",
      "14/14 [==============================] - 0s 14ms/step - loss: 0.5538 - accuracy: 0.7185 - f1_score: 0.7691\n",
      "Epoch 11/50\n",
      "14/14 [==============================] - 0s 14ms/step - loss: 0.5428 - accuracy: 0.7230 - f1_score: 0.7757\n",
      "Epoch 12/50\n",
      "14/14 [==============================] - 0s 14ms/step - loss: 0.5327 - accuracy: 0.7174 - f1_score: 0.7658\n",
      "Epoch 13/50\n",
      "14/14 [==============================] - 0s 14ms/step - loss: 0.5288 - accuracy: 0.7480 - f1_score: 0.7956\n",
      "Epoch 14/50\n",
      "14/14 [==============================] - 0s 14ms/step - loss: 0.5263 - accuracy: 0.7480 - f1_score: 0.7960\n",
      "Epoch 15/50\n",
      "14/14 [==============================] - 0s 16ms/step - loss: 0.5223 - accuracy: 0.7514 - f1_score: 0.7978\n",
      "Epoch 16/50\n",
      "14/14 [==============================] - 0s 14ms/step - loss: 0.5156 - accuracy: 0.7469 - f1_score: 0.7967\n",
      "Epoch 17/50\n",
      "14/14 [==============================] - 0s 14ms/step - loss: 0.5049 - accuracy: 0.7548 - f1_score: 0.8015\n",
      "Epoch 18/50\n",
      "14/14 [==============================] - 0s 14ms/step - loss: 0.4901 - accuracy: 0.7605 - f1_score: 0.8066\n",
      "Epoch 19/50\n",
      "14/14 [==============================] - 0s 14ms/step - loss: 0.4998 - accuracy: 0.7560 - f1_score: 0.8026\n",
      "Epoch 20/50\n",
      "14/14 [==============================] - 0s 14ms/step - loss: 0.4716 - accuracy: 0.7753 - f1_score: 0.8180\n",
      "Epoch 21/50\n",
      "14/14 [==============================] - 0s 14ms/step - loss: 0.4861 - accuracy: 0.7662 - f1_score: 0.8117\n",
      "Epoch 22/50\n",
      "14/14 [==============================] - 0s 14ms/step - loss: 0.4796 - accuracy: 0.7798 - f1_score: 0.8227\n",
      "Epoch 23/50\n",
      "14/14 [==============================] - 0s 14ms/step - loss: 0.4675 - accuracy: 0.7809 - f1_score: 0.8241\n",
      "Epoch 24/50\n",
      "14/14 [==============================] - 0s 14ms/step - loss: 0.4646 - accuracy: 0.7832 - f1_score: 0.8230\n",
      "Epoch 25/50\n",
      "14/14 [==============================] - 0s 14ms/step - loss: 0.4659 - accuracy: 0.7639 - f1_score: 0.8129\n",
      "Epoch 26/50\n",
      "14/14 [==============================] - 0s 14ms/step - loss: 0.4451 - accuracy: 0.7980 - f1_score: 0.8382\n",
      "Epoch 27/50\n",
      "14/14 [==============================] - 0s 14ms/step - loss: 0.4498 - accuracy: 0.7877 - f1_score: 0.8276\n",
      "Epoch 28/50\n",
      "14/14 [==============================] - 0s 14ms/step - loss: 0.4476 - accuracy: 0.7889 - f1_score: 0.8300\n",
      "Epoch 29/50\n",
      "14/14 [==============================] - 0s 15ms/step - loss: 0.4373 - accuracy: 0.7991 - f1_score: 0.8392\n",
      "Epoch 30/50\n",
      "14/14 [==============================] - 0s 14ms/step - loss: 0.4443 - accuracy: 0.7911 - f1_score: 0.8327\n",
      "Epoch 31/50\n",
      "14/14 [==============================] - 0s 14ms/step - loss: 0.4317 - accuracy: 0.8059 - f1_score: 0.8466\n",
      "Epoch 32/50\n",
      "14/14 [==============================] - 0s 14ms/step - loss: 0.4294 - accuracy: 0.8002 - f1_score: 0.8373\n",
      "Epoch 33/50\n",
      "14/14 [==============================] - 0s 14ms/step - loss: 0.4305 - accuracy: 0.7934 - f1_score: 0.8360\n",
      "Epoch 34/50\n",
      "14/14 [==============================] - 0s 14ms/step - loss: 0.4251 - accuracy: 0.7934 - f1_score: 0.8309\n",
      "Epoch 34: early stopping\n",
      "7/7 [==============================] - 0s 7ms/step\n"
     ]
    }
   ],
   "source": [
    "# set random seed\n",
    "np.random.seed(462023)\n",
    "tf.random.set_seed(482023)\n",
    "\n",
    "# define F1 metric\n",
    "f1_score_metric = F1Score(num_classes = 1, threshold = 0.5)\n",
    "\n",
    "# penalty hyperparameter\n",
    "RATE = 0.1\n",
    "\n",
    "# define model architecture\n",
    "model3 = models.Sequential([\n",
    "    Dense(512, activation = 'relu', input_shape = (X_train.shape[1], )),\n",
    "    Dropout(RATE),\n",
    "    Dense(256, activation = 'relu'),\n",
    "    Dropout(RATE),\n",
    "    Dense(128, activation = 'relu'),\n",
    "    Dropout(RATE),\n",
    "    Dense(64, activation = 'relu'),\n",
    "    Dropout(RATE),\n",
    "    Dense(32, activation = 'relu'),\n",
    "    Dropout(RATE),\n",
    "    Dense(1, activation = 'sigmoid')\n",
    "])\n",
    "\n",
    "# compile model\n",
    "model3.compile(optimizer = 'rmsprop',\n",
    "               loss = 'binary_crossentropy',\n",
    "               metrics = ['accuracy', f1_score_metric])\n",
    "\n",
    "# model summary\n",
    "model3.summary()\n",
    "\n",
    "# number of epochs\n",
    "EPOCHS = 50\n",
    "\n",
    "# early stopping criteria\n",
    "earlyStop = EarlyStopping(monitor = 'f1_score', mode = 'max', verbose = 1, patience = 3)\n",
    "\n",
    "# train model\n",
    "trained3 = model3.fit(X_train, \n",
    "                      y_train, \n",
    "                      epochs = EPOCHS, \n",
    "                      batch_size = 64, \n",
    "                      callbacks = earlyStop,\n",
    "                      verbose = 1)\n",
    "\n",
    "# predict on validation set\n",
    "pred3 = model3.predict(X_valid)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsci",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
