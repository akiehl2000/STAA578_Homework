{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2 Final Models (Loan Prediction)\n",
    "\n",
    "Adam Kiehl  \n",
    "4/23/23"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/akiehl/miniconda3/envs/dsci/lib/python3.8/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "/Users/akiehl/miniconda3/envs/dsci/lib/python3.8/site-packages/tensorflow_addons/utils/ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.10.0 and strictly below 2.13.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.9.2 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# import analysis packages\n",
    "import keras\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras import models\n",
    "from keras.regularizers import l2\n",
    "from keras.utils import to_categorical\n",
    "import keras.backend as back\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.compose import make_column_selector\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow_addons.metrics import F1Score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data from .csvs\n",
    "trainDF = pd.read_csv('./loan_train.csv')\n",
    "testDF = pd.read_csv('./loan_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate response/prediction columns\n",
    "trainResp = np.where(trainDF['MIS_Status'] == 'P I F', 1, 0)\n",
    "trainDF.drop('MIS_Status', axis = 1, inplace = True)\n",
    "testIDs = testDF['CustomerId']\n",
    "testDF.drop('CustomerId', axis = 1, inplace = True)\n",
    "\n",
    "# combine data sets for preprocessing\n",
    "trainDF['source'] = 'train'\n",
    "testDF['source'] = 'test'\n",
    "fullDF = pd.concat([trainDF, testDF], axis = 0)\n",
    "\n",
    "# factor categorical predictors\n",
    "fullDF['NAICS'] = fullDF['NAICS'].apply(lambda x: str(x))\n",
    "fullDF['NewExist'] = fullDF['NewExist'].apply(lambda x: str(x))\n",
    "fullDF['UrbanRural'] = fullDF['UrbanRural'].apply(lambda x: str(x))\n",
    "fullDF['RevLineCr'] = np.where(fullDF['RevLineCr'] == 'Y', 'Y', 'N')\n",
    "fullDF['LowDoc'] = np.where(fullDF['LowDoc'] == 'Y', 'Y', 'N')\n",
    "fullDF['New'] = fullDF['New'].apply(lambda x: str(x))\n",
    "fullDF['RealEstate'] = fullDF['RealEstate'].apply(lambda x: str(x))\n",
    "fullDF['Recession'] = fullDF['Recession'].apply(lambda x: str(x))\n",
    "\n",
    "# selected predictors\n",
    "predictors = ['NAICS', 'Term', 'NoEmp', 'CreateJob', 'RetainedJob', 'UrbanRural', 'RevLineCr', 'LowDoc', 'DisbursementGross', 'GrAppv', 'New', 'RealEstate', 'Portion', 'Recession']\n",
    "src = fullDF['source']\n",
    "fullDF = fullDF[predictors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale numeric predictors and encode categorical predictors\n",
    "findNumPredictors = make_column_selector(dtype_exclude = object)\n",
    "findCatPredictors = make_column_selector(dtype_include = object)\n",
    "transform = make_column_transformer((MinMaxScaler(), findNumPredictors),\n",
    "                                    (OneHotEncoder(), findCatPredictors))\n",
    "\n",
    "# get new column names\n",
    "colNames = transform.fit(fullDF).get_feature_names_out()\n",
    "\n",
    "# transform data\n",
    "modelDF = pd.DataFrame(transform.fit_transform(fullDF), columns = colNames)\n",
    "\n",
    "# split data into training, validation, and test sets\n",
    "modelTrain = modelDF.loc[np.where(src == 'train')]\n",
    "modelTest = modelDF.iloc[np.where(src == 'test')]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosted Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuned hyperparameters\n",
    "L = 0.1\n",
    "D = 1\n",
    "\n",
    "# fit gradient boosted model\n",
    "model1 = GradientBoostingClassifier(learning_rate = L, max_depth = D, n_estimators = 1000, random_state = 4192023)\n",
    "model1.fit(modelTrain, trainResp)\n",
    "\n",
    "# predict on validation set\n",
    "pred1 = model1.predict(modelTest)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n",
      "\n",
      "systemMemory: 8.00 GB\n",
      "maxCacheSize: 2.67 GB\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 512)               23040     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 197,633\n",
      "Trainable params: 197,633\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-23 13:35:24.432127: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 3s 52ms/step - loss: 0.6300 - accuracy: 0.6044 - f1_score: 0.7187\n",
      "Epoch 2/50\n",
      "18/18 [==============================] - 0s 14ms/step - loss: 0.6080 - accuracy: 0.6706 - f1_score: 0.7348\n",
      "Epoch 3/50\n",
      "18/18 [==============================] - 0s 13ms/step - loss: 0.5920 - accuracy: 0.6897 - f1_score: 0.7518\n",
      "Epoch 4/50\n",
      "18/18 [==============================] - 0s 13ms/step - loss: 0.5767 - accuracy: 0.6942 - f1_score: 0.7549\n",
      "Epoch 5/50\n",
      "18/18 [==============================] - 0s 14ms/step - loss: 0.5860 - accuracy: 0.6869 - f1_score: 0.7569\n",
      "Epoch 6/50\n",
      "18/18 [==============================] - 0s 13ms/step - loss: 0.5677 - accuracy: 0.7042 - f1_score: 0.7641\n",
      "Epoch 7/50\n",
      "18/18 [==============================] - 0s 13ms/step - loss: 0.5613 - accuracy: 0.7069 - f1_score: 0.7740\n",
      "Epoch 8/50\n",
      "18/18 [==============================] - 0s 13ms/step - loss: 0.5487 - accuracy: 0.7214 - f1_score: 0.7830\n",
      "Epoch 9/50\n",
      "18/18 [==============================] - 0s 13ms/step - loss: 0.5483 - accuracy: 0.7151 - f1_score: 0.7764\n",
      "Epoch 10/50\n",
      "18/18 [==============================] - 0s 14ms/step - loss: 0.5386 - accuracy: 0.7468 - f1_score: 0.8009\n",
      "Epoch 11/50\n",
      "18/18 [==============================] - 0s 15ms/step - loss: 0.5444 - accuracy: 0.7196 - f1_score: 0.7801\n",
      "Epoch 12/50\n",
      "18/18 [==============================] - 0s 13ms/step - loss: 0.5210 - accuracy: 0.7523 - f1_score: 0.8037\n",
      "Epoch 13/50\n",
      "18/18 [==============================] - 0s 14ms/step - loss: 0.5254 - accuracy: 0.7396 - f1_score: 0.7963\n",
      "Epoch 14/50\n",
      "18/18 [==============================] - 0s 15ms/step - loss: 0.5113 - accuracy: 0.7604 - f1_score: 0.8136\n",
      "Epoch 15/50\n",
      "18/18 [==============================] - 0s 14ms/step - loss: 0.4975 - accuracy: 0.7731 - f1_score: 0.8261\n",
      "Epoch 16/50\n",
      "18/18 [==============================] - 0s 14ms/step - loss: 0.4942 - accuracy: 0.7704 - f1_score: 0.8242\n",
      "Epoch 17/50\n",
      "18/18 [==============================] - 0s 13ms/step - loss: 0.5064 - accuracy: 0.7532 - f1_score: 0.8085\n",
      "Epoch 18/50\n",
      "18/18 [==============================] - 0s 13ms/step - loss: 0.4722 - accuracy: 0.7813 - f1_score: 0.8314\n",
      "Epoch 19/50\n",
      "18/18 [==============================] - 0s 13ms/step - loss: 0.4876 - accuracy: 0.7786 - f1_score: 0.8260\n",
      "Epoch 20/50\n",
      "18/18 [==============================] - 0s 14ms/step - loss: 0.4695 - accuracy: 0.7886 - f1_score: 0.8346\n",
      "Epoch 21/50\n",
      "18/18 [==============================] - 0s 13ms/step - loss: 0.4748 - accuracy: 0.7722 - f1_score: 0.8229\n",
      "Epoch 22/50\n",
      "18/18 [==============================] - 0s 13ms/step - loss: 0.4711 - accuracy: 0.7750 - f1_score: 0.8251\n",
      "Epoch 23/50\n",
      "18/18 [==============================] - 0s 13ms/step - loss: 0.4668 - accuracy: 0.7740 - f1_score: 0.8245\n",
      "Epoch 23: early stopping\n",
      "32/32 [==============================] - 0s 7ms/step\n"
     ]
    }
   ],
   "source": [
    "# set random seed\n",
    "np.random.seed(462023)\n",
    "tf.random.set_seed(482023)\n",
    "\n",
    "# define F1 metric\n",
    "f1_score_metric = F1Score(num_classes = 1, threshold = 0.5)\n",
    "\n",
    "# penalty hyperparameter\n",
    "RATE = 0.1\n",
    "\n",
    "# define model architecture\n",
    "model2 = models.Sequential([\n",
    "    Dense(512, activation = 'relu', input_shape = (modelTrain.shape[1], )),\n",
    "    Dropout(RATE),\n",
    "    Dense(256, activation = 'relu'),\n",
    "    Dropout(RATE),\n",
    "    Dense(128, activation = 'relu'),\n",
    "    Dropout(RATE),\n",
    "    Dense(64, activation = 'relu'),\n",
    "    Dropout(RATE),\n",
    "    Dense(32, activation = 'relu'),\n",
    "    Dropout(RATE),\n",
    "    Dense(1, activation = 'sigmoid')\n",
    "])\n",
    "\n",
    "# compile model\n",
    "model2.compile(optimizer = 'rmsprop',\n",
    "               loss = 'binary_crossentropy',\n",
    "               metrics = ['accuracy', f1_score_metric])\n",
    "\n",
    "# model summary\n",
    "model2.summary()\n",
    "\n",
    "# number of epochs\n",
    "EPOCHS = 50\n",
    "\n",
    "# early stopping criteria\n",
    "earlyStop = EarlyStopping(monitor = 'f1_score', mode = 'max', verbose = 1, patience = 3)\n",
    "\n",
    "# train model\n",
    "trained2 = model2.fit(modelTrain, \n",
    "                      trainResp, \n",
    "                      epochs = EPOCHS, \n",
    "                      batch_size = 64, \n",
    "                      callbacks = earlyStop,\n",
    "                      verbose = 1)\n",
    "\n",
    "# predict on validation set\n",
    "pred2 = model2.predict(modelTest)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsci",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
