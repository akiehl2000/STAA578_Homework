{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1 (Income Prediction)\n",
    "\n",
    "The goal of this assignment is to predict whether an individual makes more or less than $50,000 given demographic data. A data dictionary was provided:  \n",
    "\n",
    "age: continuous.  \n",
    "workclass: Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked.  \n",
    "fnlwgt: continuous.  \n",
    "education: Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool.  \n",
    "education-num: continuous.  \n",
    "marital-status: Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse.  \n",
    "occupation: Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces.  \n",
    "relationship: Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried.  \n",
    "race: White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black.  \n",
    "sex: Female, Male.  \n",
    "capital-gain: continuous.  \n",
    "capital-loss: continuous.  \n",
    "hours-per-week: continuous.  \n",
    "native-country: United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&Tobago, Peru, Hong, Holand-Netherlands. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analysis packages\n",
    "import keras\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras import models\n",
    "from keras.regularizers import l2\n",
    "from keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "from scipy import stats\n",
    "import sklearn\n",
    "from sklearn import compose\n",
    "from sklearn.compose import make_column_selector\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import ensemble\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import statsmodels.api as sm\n",
    "import warnings\n",
    "import tensorflow as tf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read training data from .csv\n",
    "trainDF = pd.read_csv('./adult.csv')\n",
    "\n",
    "# drop id column (wont be used for modeling)\n",
    "trainDF.drop('id', axis = 1, inplace = True)\n",
    "\n",
    "# check data frame dimension\n",
    "display(trainDF.shape)\n",
    "\n",
    "# peek at data\n",
    "display(trainDF.head())\n",
    "\n",
    "# check data types\n",
    "display(trainDF.info())\n",
    "\n",
    "# check for missing data\n",
    "display(trainDF.isna().sum())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numeric Predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize numeric data\n",
    "display(trainDF.describe())\n",
    "\n",
    "# suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# numeric analysis function\n",
    "def studyNumPredictor(pred):\n",
    "    # boxplot by income level\n",
    "    plot = sns.boxplot(x = trainDF['income'], y = trainDF[pred])\n",
    "    plot.set(xlabel = 'Income Level', title = f\"{pred} Comparison\")\n",
    "    plt.show()\n",
    "\n",
    "    # single logistic regression\n",
    "    logFit = LogisticRegression().fit(np.array(trainDF[pred]).reshape(-1, 1), trainDF['income'])\n",
    "    logFit = sm.Logit(pd.get_dummies(trainDF['income'])['>50K'], trainDF[pred]).fit(disp = 0)\n",
    "    display(logFit.pvalues)\n",
    "\n",
    "\n",
    "# identify and study numeric predictors\n",
    "findNumPredictors = make_column_selector(dtype_include = int)\n",
    "for pred in trainDF[findNumPredictors].columns:\n",
    "    studyNumPredictor(pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical analysis function\n",
    "def studyCatPredictor(pred):\n",
    "    # construct contingency table\n",
    "    contingency = pd.crosstab(trainDF['income'], trainDF[pred])\n",
    "    display(contingency)\n",
    "\n",
    "    # chi squared test for independence\n",
    "    print(f\"Chi-squared test p-value: {stats.chi2_contingency(contingency)[1]}\")\n",
    "\n",
    "# identify and study categorical predictors\n",
    "findCatPredictors = make_column_selector(dtype_include = object)\n",
    "for pred in trainDF.drop('income', axis = 1, inplace = False)[findCatPredictors].columns:\n",
    "    studyCatPredictor(pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale numeric predictors and encode categorical predictors\n",
    "findNumPredictors = make_column_selector(dtype_include = int)\n",
    "findCatPredictors = make_column_selector(dtype_include = object)\n",
    "transform = make_column_transformer((MinMaxScaler(), findNumPredictors),\n",
    "                                    (OneHotEncoder(drop = 'first'), findCatPredictors))\n",
    "\n",
    "# get new column names\n",
    "colNames = transform.fit(trainDF).get_feature_names_out()\n",
    "\n",
    "# transform data\n",
    "modelDF = pd.DataFrame.sparse.from_spmatrix(transform.fit_transform(trainDF), columns = colNames)\n",
    "\n",
    "# set random seed\n",
    "np.random.seed(432023)\n",
    "\n",
    "# split data into predictors and response\n",
    "resp = modelDF['onehotencoder__income_>50K'].rename('income')\n",
    "modelDF.drop('onehotencoder__income_>50K', axis = 1, inplace = True)\n",
    "\n",
    "# split data into training and validation sets\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(modelDF, resp, test_size = 0.2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Fitting"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A KNN model using 9 neighbors achieved 83.12% cross-validation accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seed\n",
    "np.random.seed(482023)\n",
    "\n",
    "# tune KNN model\n",
    "Kvalues = range(1, 20)\n",
    "knnFit = KNeighborsClassifier()\n",
    "knnTune = GridSearchCV(knnFit,\n",
    "                       param_grid = {'n_neighbors': Kvalues},\n",
    "                       cv = 5,\n",
    "                       n_jobs = -1,\n",
    "                       verbose = 2)\n",
    "knnTune.fit(modelDF, resp)\n",
    "K = knnTune.best_params_['n_neighbors']\n",
    "\n",
    "# fit KNN model\n",
    "knnFit = KNeighborsClassifier(n_neighbors = K)\n",
    "cvResults = cross_val_score(knnFit,\n",
    "                            modelDF,\n",
    "                            resp,\n",
    "                            cv = 10,\n",
    "                            n_jobs = -1,\n",
    "                            verbose = 2)\n",
    "\n",
    "# cross-validation accuracy\n",
    "print(f\"Neighbors considered: {K}\")\n",
    "print(f\"Cross-validation accuracy: {np.mean(cvResults).round(4)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A random forest model using 17 features achieved 85.64% cross-validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seed\n",
    "np.random.seed(432023)\n",
    "\n",
    "# tune random forest model\n",
    "Mvalues = range(1, 20)\n",
    "rfFit = RandomForestClassifier()\n",
    "rfTune = GridSearchCV(rfFit,\n",
    "                      param_grid = {'max_features': Mvalues},\n",
    "                      cv = 5,\n",
    "                      n_jobs = -1,\n",
    "                      verbose = 2)\n",
    "rfTune.fit(modelDF, resp)\n",
    "M = rfTune.best_params_['max_features']\n",
    "\n",
    "# fit random forest model\n",
    "rfFit = RandomForestClassifier(max_features = M,\n",
    "                               n_estimators = 1000)\n",
    "cvResults = cross_val_score(rfFit,\n",
    "                            modelDF,\n",
    "                            resp,\n",
    "                            cv = 10,\n",
    "                            n_jobs = -1,\n",
    "                            verbose = 2)\n",
    "\n",
    "# cross-validation accuracy\n",
    "print(f\"Features considered: {M}\")\n",
    "print(f\"Cross-validation accuracy: {np.mean(cvResults).round(4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model fitting and evaluation function\n",
    "def EvaluateModel(model, early = False, criteria = 'val_loss', opt = 'min'):\n",
    "    # set random seed\n",
    "    np.random.seed(462023)\n",
    "    tf.random.set_seed(482023)\n",
    "\n",
    "    # compile model\n",
    "    model.compile(optimizer = 'rmsprop',\n",
    "                loss = 'categorical_crossentropy',\n",
    "                metrics = ['accuracy'])\n",
    "\n",
    "    # model summary\n",
    "    model.summary()\n",
    "\n",
    "    # number of epochs\n",
    "    EPOCHS = 50\n",
    "\n",
    "    if early:\n",
    "        # early stopping criteria\n",
    "        earlyStop = EarlyStopping(monitor = criteria, mode = opt, verbose = 1, patience = 3)\n",
    "\n",
    "        # train model\n",
    "        trained = model.fit(X_train, \n",
    "                            to_categorical(y_train), \n",
    "                            epochs = EPOCHS, \n",
    "                            batch_size = 128, \n",
    "                            validation_split = 0.2,\n",
    "                            callbacks = earlyStop,\n",
    "                            verbose = 1)\n",
    "    else:\n",
    "        # train model\n",
    "        trained = model.fit(X_train, \n",
    "                            to_categorical(y_train), \n",
    "                            epochs = EPOCHS, \n",
    "                            batch_size = 128, \n",
    "                            validation_split = 0.2,\n",
    "                            verbose = 1)\n",
    "\n",
    "    # prepare model evaluation\n",
    "    acc = trained.history['accuracy']\n",
    "    val_acc = trained.history['val_accuracy']\n",
    "    loss = trained.history['loss']\n",
    "    val_loss = trained.history['val_loss']\n",
    "    epochs = range(1, len(acc) + 1)\n",
    "\n",
    "    # plot training accuracy\n",
    "    plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "    plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.legend()\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "    plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple MLP with one hidden layer was fit using 50 epochs and a batch size of 128. This model achieved a training accuracy of 88.03% and a validation accuracy of 85.16%. This model displayed clear evidence of overfitting with validation accuracy flattening out after about 10 epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model architecture\n",
    "model = models.Sequential([\n",
    "    Dense(512, activation = 'relu', input_shape = (X_train.shape[1], )),\n",
    "    Dense(2, activation = 'sigmoid')\n",
    "])\n",
    "\n",
    "# evaluate model\n",
    "EvaluateModel(model, False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous model was expanded to include two hidden layers. This model achieved a training accuracy of 90.64% and a validation accuracy of 83.38%. This model alsodisplayed clear evidence of overfitting with validation accuracy decreasing after about 10 epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model architecture\n",
    "model = models.Sequential([\n",
    "    Dense(512, activation = 'relu', input_shape = (X_train.shape[1], )),\n",
    "    Dense(256, activation = 'relu'),\n",
    "    Dense(2, activation = 'sigmoid')\n",
    "])\n",
    "\n",
    "# evaluate model\n",
    "EvaluateModel(model, False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original model was again expanded to include three hidden layers. This model achieved a training accuracy of 90.87% and a validation accuracy of 83.45%. This model also displayed clear evidence of overfitting with validation accuracy decreasing after about 10 epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model architecture\n",
    "model = models.Sequential([\n",
    "    Dense(512, activation = 'relu', input_shape = (X_train.shape[1], )),\n",
    "    Dense(256, activation = 'relu'),\n",
    "    Dense(128, activation = 'relu'),\n",
    "    Dense(2, activation = 'sigmoid')\n",
    "])\n",
    "\n",
    "# evaluate model\n",
    "EvaluateModel(model, False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to clear evidence of overfitting, an early stopping algorithm (based on validation loss) was applied to the original model with one hidden layer. Stopping after 7 epochs, this model achieved a training accuracy of 85.44% and a validation accuracy 85.14%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model architecture\n",
    "model = models.Sequential([\n",
    "    Dense(512, activation = 'relu', input_shape = (X_train.shape[1], )),\n",
    "    Dense(2, activation = 'sigmoid')\n",
    "])\n",
    "\n",
    "# evaluate model\n",
    "EvaluateModel(model, True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another early stopping algorithm (based on validation accuracy) was applied to the original model with one hidden layer. Stopping after 11 epochs, this model achieved a training accuracy of 85.80% and a validation accuracy 84.89%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model architecture\n",
    "model = models.Sequential([\n",
    "    Dense(512, activation = 'relu', input_shape = (X_train.shape[1], )),\n",
    "    Dense(2, activation = 'sigmoid')\n",
    "])\n",
    "\n",
    "# evaluate model\n",
    "EvaluateModel(model, True, 'val_accuracy', 'max')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An early stopping algorithm (based on validation loss) was applied to the MLP with three hidden layers. Stopping after 7 epochs, this model achieved a training accuracy of 85.94% and a valiation accuracy of 84.76%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model architecture\n",
    "model = models.Sequential([\n",
    "    Dense(512, activation = 'relu', input_shape = (X_train.shape[1], )),\n",
    "    Dense(256, activation = 'relu'),\n",
    "    Dense(128, activation = 'relu'),\n",
    "    Dense(2, activation = 'sigmoid')\n",
    "])\n",
    "\n",
    "# evaluate model\n",
    "EvaluateModel(model, True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropout regularization with a rate of 0.25 was applied to the MLP with three hidden layers. Stopping after 8 epochs, this model achieved a training accuracy of 85.65% and a validation accuracy of 85.18%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropout rate\n",
    "RATE = 0.25\n",
    "\n",
    "# define model architecture\n",
    "model = models.Sequential([\n",
    "    Dense(512, activation = 'relu', input_shape = (X_train.shape[1], )),\n",
    "    Dropout(rate = RATE),\n",
    "    Dense(256, activation = 'relu'),\n",
    "    Dropout(rate = RATE),\n",
    "    Dense(128, activation = 'relu'),\n",
    "    Dropout(rate = RATE),\n",
    "    Dense(2, activation = 'sigmoid')\n",
    "])\n",
    "\n",
    "EvaluateModel(model, True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropout regularization with a rate of 0.5 was applied to the MLP with three hidden layers. Stopping after 8 epochs, this model achieved a training accuracy of 84.95% and a validation accuracy of 84.64%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropout rate\n",
    "RATE = 0.5\n",
    "\n",
    "# define model architecture\n",
    "model = models.Sequential([\n",
    "    Dense(512, activation = 'relu', input_shape = (X_train.shape[1], )),\n",
    "    Dropout(rate = RATE),\n",
    "    Dense(256, activation = 'relu'),\n",
    "    Dropout(rate = RATE),\n",
    "    Dense(128, activation = 'relu'),\n",
    "    Dropout(rate = RATE),\n",
    "    Dense(2, activation = 'sigmoid')\n",
    "])\n",
    "\n",
    "# evaluate model\n",
    "EvaluateModel(model, True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An L2 normalizing penalty (strenght of 0.001) was applied to the MLP with three hidden layers. Stopping after 27 epochs, this model achieved a training accuracy of 85.80% and a validation accuracy of 85.49%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropout rate\n",
    "PENALTY = 0.001\n",
    "\n",
    "# define model architecture\n",
    "model = models.Sequential([\n",
    "    Dense(512, activation = 'relu', kernel_regularizer = l2(PENALTY), input_shape = (X_train.shape[1], )),\n",
    "    Dense(256, activation = 'relu', kernel_regularizer = l2(PENALTY)),\n",
    "    Dense(128, activation = 'relu', kernel_regularizer = l2(PENALTY)),\n",
    "    Dense(2, activation = 'sigmoid')\n",
    "])\n",
    "\n",
    "# evaluate model\n",
    "EvaluateModel(model, True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An L2 normalizing penalty (strenght of 0.01) was applied to the MLP with three hidden layers. Stopping after 17 epochs, this model achieved a training accuracy of 83.53% and a validation accuracy of 83.69."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropout rate\n",
    "PENALTY = 0.01\n",
    "\n",
    "# define model architecture\n",
    "model = models.Sequential([\n",
    "    Dense(512, activation = 'relu', kernel_regularizer = l2(PENALTY), input_shape = (X_train.shape[1], )),\n",
    "    Dense(256, activation = 'relu', kernel_regularizer = l2(PENALTY)),\n",
    "    Dense(128, activation = 'relu', kernel_regularizer = l2(PENALTY)),\n",
    "    Dense(2, activation = 'sigmoid')\n",
    "])\n",
    "\n",
    "# evaluate model\n",
    "EvaluateModel(model, True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
