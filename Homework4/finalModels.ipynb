{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 4 Final Models (Text Classification)\n",
    "\n",
    "Adam Kiehl  \n",
    "5/7/2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import analysis packages\n",
    "import gensim\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import keras\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Dense, Dropout, Embedding, Flatten, SimpleRNN, TextVectorization\n",
    "from keras.models import Sequential\n",
    "from keras.regularizers import l2\n",
    "from keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow_addons.metrics import F1Score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data from .csv files\n",
    "trainDF = pd.read_csv('./ibotta_train.csv')\n",
    "testDF = pd.read_csv('./ibotta_test.csv')\n",
    "\n",
    "# combine data sets for preprocessing\n",
    "trainDF['origin'] = 'train'\n",
    "testDF['origin'] = 'test'\n",
    "fullDF = pd.concat([trainDF, testDF])\n",
    "\n",
    "# text cleaning\n",
    "fullDF['Brand_name'].where(-fullDF['Brand_name'].isna(), '', inplace = True)\n",
    "fullDF['Brand_name'] = fullDF['Brand_name'].apply(lambda x: x.lower().replace(\"'\", \"\").replace(\",\", \"\").replace(\":\", \"\").replace(\"-\", \"\").replace(\".\", \"\"))\n",
    "fullDF['Name'] = fullDF['Name'].apply(lambda x: x.lower().replace(\"'\", \"\").replace(\",\", \"\").replace(\":\", \"\").replace(\"-\", \"\").replace(\".\", \"\"))\n",
    "\n",
    "# combine brand and name fields\n",
    "fullDF['brandAlready'] = fullDF.apply(lambda x: x['Name'].find(x['Brand_name']), axis = 1)\n",
    "fullDF.loc[fullDF.brandAlready == -1, 'Name'] = fullDF.loc[fullDF.brandAlready == -1, 'Brand_name'] + \\\n",
    "    ' ' + fullDF.loc[fullDF.brandAlready == -1, 'Name']\n",
    "fullDF.drop('brandAlready', axis = 1, inplace = True)\n",
    "\n",
    "# split data\n",
    "trainDF = pd.DataFrame(fullDF.loc[fullDF['origin'] == 'train'].drop('origin', axis = 1))\n",
    "testDF = pd.DataFrame(fullDF.loc[fullDF['origin'] == 'test'].drop(['origin', 'Category'], axis = 1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find total number of unique words\n",
    "unique_words = np.unique(np.array(' '.join(np.array(fullDF['Name'])).split(' ')))\n",
    "max_length = len(unique_words)\n",
    "\n",
    "# initialize empty dataframe\n",
    "wordBag = pd.DataFrame(np.zeros((len(fullDF), max_length)), \n",
    "                       columns = unique_words)\n",
    "\n",
    "# loop through product names\n",
    "for i, productName in enumerate(fullDF['Name'].apply(lambda x: x.split(' '))):\n",
    "    # loop through words in name\n",
    "    for word in productName:\n",
    "        # identify word presence\n",
    "        wordBag.loc[i, word] = 1\n",
    "\n",
    "# split word bag\n",
    "trainDFwordBag = wordBag.loc[0:7999]\n",
    "testDFwordBag = wordBag.loc[8000:9999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras tokenizer function\n",
    "def textVecGen(size):\n",
    "    # train keras tokenizer\n",
    "    tokenizer = TextVectorization(max_tokens = size, \n",
    "                                  output_sequence_length = size)\n",
    "    tokenizer.adapt(fullDF['Name'])\n",
    "\n",
    "    # vectorize data\n",
    "    textVecDF = pd.DataFrame(tokenizer(fullDF['Name']))\n",
    "    \n",
    "    # split vectorized data\n",
    "    trainDFtextVec = textVecDF.loc[0:7999]\n",
    "    testDFtextVec = textVecDF.loc[8000:9999]\n",
    "\n",
    "    return(trainDFtextVec, testDFtextVec)\n",
    "\n",
    "# generate vectorized data\n",
    "trainDFtextVec1000, testDFtextVec1000 = textVecGen(1000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "63/63 [==============================] - 2s 23ms/step - loss: 0.6617 - accuracy: 0.7756 - f1_score: 0.7751\n",
      "Epoch 2/100\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.1072 - accuracy: 0.9715 - f1_score: 0.9715\n",
      "Epoch 3/100\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0493 - accuracy: 0.9856 - f1_score: 0.9856\n",
      "Epoch 4/100\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0291 - accuracy: 0.9904 - f1_score: 0.9904\n",
      "Epoch 5/100\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0172 - accuracy: 0.9946 - f1_score: 0.9946\n",
      "Epoch 6/100\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0131 - accuracy: 0.9946 - f1_score: 0.9946\n",
      "Epoch 7/100\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0089 - accuracy: 0.9964 - f1_score: 0.9964\n",
      "Epoch 8/100\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0076 - accuracy: 0.9966 - f1_score: 0.9966\n",
      "Epoch 9/100\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0055 - accuracy: 0.9983 - f1_score: 0.9982\n",
      "Epoch 10/100\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0040 - accuracy: 0.9985 - f1_score: 0.9985\n",
      "Epoch 11/100\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0036 - accuracy: 0.9990 - f1_score: 0.9990\n",
      "Epoch 12/100\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0024 - accuracy: 0.9990 - f1_score: 0.9990\n",
      "Epoch 13/100\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0028 - accuracy: 0.9991 - f1_score: 0.9991\n",
      "Epoch 14/100\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0028 - accuracy: 0.9991 - f1_score: 0.9991\n",
      "Epoch 15/100\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0021 - accuracy: 0.9991 - f1_score: 0.9991\n",
      "Epoch 16/100\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0015 - accuracy: 0.9992 - f1_score: 0.9992\n",
      "Epoch 17/100\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0020 - accuracy: 0.9991 - f1_score: 0.9991\n",
      "Epoch 18/100\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0015 - accuracy: 0.9994 - f1_score: 0.9994\n",
      "Epoch 19/100\n",
      "63/63 [==============================] - 1s 21ms/step - loss: 0.0013 - accuracy: 0.9994 - f1_score: 0.9994\n",
      "Epoch 20/100\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0012 - accuracy: 0.9994 - f1_score: 0.9994\n",
      "Epoch 21/100\n",
      "63/63 [==============================] - 1s 18ms/step - loss: 0.0012 - accuracy: 0.9995 - f1_score: 0.9995\n",
      "Epoch 22/100\n",
      "63/63 [==============================] - 1s 18ms/step - loss: 0.0012 - accuracy: 0.9994 - f1_score: 0.9994\n",
      "Epoch 23/100\n",
      "63/63 [==============================] - 1s 18ms/step - loss: 0.0013 - accuracy: 0.9990 - f1_score: 0.9990\n",
      "Epoch 24/100\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 9.1478e-04 - accuracy: 0.9995 - f1_score: 0.9995\n",
      "63/63 [==============================] - 0s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "# set random seeds\n",
    "np.random.seed(542023)\n",
    "tf.random.set_seed(542023)\n",
    "\n",
    "# define model architecture\n",
    "model1 = Sequential([\n",
    "    Dense(512, activation = 'relu'),\n",
    "    Dense(256, activation = 'relu'),\n",
    "    Dense(128, activation = 'relu'),\n",
    "    Dense(64, activation = 'relu'),\n",
    "    Dense(7, activation = 'softmax')\n",
    "])\n",
    "\n",
    "# define F1 metric\n",
    "f1_score_metric = F1Score(num_classes = 7, average = 'weighted')\n",
    "\n",
    "# compile model\n",
    "model1.compile(optimizer = 'rmsprop',\n",
    "               loss = 'categorical_crossentropy',\n",
    "               metrics = ['accuracy', f1_score_metric])\n",
    "    \n",
    "# define early stopping criterion\n",
    "early = EarlyStopping(monitor = 'f1_score', mode = 'max', patience = 3)\n",
    "\n",
    "# train deep learning model\n",
    "trained1 = model1.fit(trainDFwordBag,\n",
    "                      to_categorical(trainDF['Cat_code']),\n",
    "                      epochs = 100,\n",
    "                      batch_size = 128,\n",
    "                      callbacks = early,\n",
    "                      verbose = 1)\n",
    "\n",
    "# predict on test set\n",
    "pred1 = model1.predict(testDFwordBag)\n",
    "\n",
    "# create submission data frame\n",
    "submission = pd.DataFrame({'Id': testDF['Id'], 'Cat_code': np.argmax(pred1, axis = 1).reshape(len(pred1), )})\n",
    "\n",
    "# export submission\n",
    "submission.to_csv('./submission1.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 1000, 128)         128000    \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 128000)            0         \n",
      "                                                                 \n",
      " dense_25 (Dense)            (None, 128)               16384128  \n",
      "                                                                 \n",
      " dense_26 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_27 (Dense)            (None, 7)                 455       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 16,520,839\n",
      "Trainable params: 16,520,839\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "63/63 [==============================] - 39s 608ms/step - loss: 2.3022 - accuracy: 0.3170 - f1_score: 0.2640\n",
      "Epoch 2/100\n",
      "63/63 [==============================] - 38s 609ms/step - loss: 1.1902 - accuracy: 0.5882 - f1_score: 0.5200\n",
      "Epoch 3/100\n",
      "63/63 [==============================] - 9s 129ms/step - loss: 1.7783 - accuracy: 0.3124 - f1_score: 0.2097\n",
      "Epoch 4/100\n",
      "63/63 [==============================] - 5s 83ms/step - loss: 1.8049 - accuracy: 0.2657 - f1_score: 0.1469\n",
      "Epoch 5/100\n",
      "63/63 [==============================] - 5s 83ms/step - loss: 1.7707 - accuracy: 0.2940 - f1_score: 0.1336\n",
      "63/63 [==============================] - 1s 13ms/step\n"
     ]
    }
   ],
   "source": [
    "# set random seeds\n",
    "np.random.seed(542023)\n",
    "tf.random.set_seed(542023)\n",
    "\n",
    "# define model architecture\n",
    "model2 = Sequential([\n",
    "    Embedding(1000, 128, input_shape = (1000, )),\n",
    "    Flatten(),\n",
    "    Dense(128, activation = 'relu'),\n",
    "    Dense(64, activation = 'relu'),\n",
    "    Dense(7, activation = 'softmax')\n",
    "])\n",
    "\n",
    "# print model summary\n",
    "model2.summary()\n",
    "\n",
    "# define F1 metric\n",
    "f1_score_metric = F1Score(num_classes = 7, average = 'weighted')\n",
    "\n",
    "# compile model\n",
    "model2.compile(optimizer = 'rmsprop',\n",
    "               loss = 'categorical_crossentropy',\n",
    "               metrics = ['accuracy', f1_score_metric])\n",
    "    \n",
    "# define early stopping criterion\n",
    "early = EarlyStopping(monitor = 'f1_score', mode = 'max', patience = 3)\n",
    "\n",
    "# train deep learning model\n",
    "trained2 = model2.fit(trainDFtextVec1000,\n",
    "                      to_categorical(trainDF['Cat_code']),\n",
    "                      epochs = 100,\n",
    "                      batch_size = 128,\n",
    "                      callbacks = early,\n",
    "                      verbose = 1)\n",
    "\n",
    "# predict on test set\n",
    "pred2 = model2.predict(testDFtextVec1000)\n",
    "\n",
    "# create submission data frame\n",
    "submission = pd.DataFrame({'Id': testDF['Id'], 'Cat_code': np.argmax(pred2, axis = 1).reshape(len(pred2), )})\n",
    "\n",
    "# export submission\n",
    "submission.to_csv('./submission2.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsci",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
