{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 4 Final Models (Text Classification)\n",
    "\n",
    "Adam Kiehl  \n",
    "5/7/2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import analysis packages\n",
    "import gensim\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import keras\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Dense, Dropout, Embedding, Flatten, SimpleRNN, TextVectorization\n",
    "from keras.models import Sequential\n",
    "from keras.regularizers import l2\n",
    "from keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow_addons.metrics import F1Score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data from .csv files\n",
    "trainDF = pd.read_csv('./ibotta_train.csv')\n",
    "testDF = pd.read_csv('./ibotta_test.csv')\n",
    "\n",
    "# combine data sets for preprocessing\n",
    "trainDF['origin'] = 'train'\n",
    "testDF['origin'] = 'test'\n",
    "fullDF = pd.concat([trainDF, testDF])\n",
    "\n",
    "# text cleaning\n",
    "fullDF['Brand_name'].where(-fullDF['Brand_name'].isna(), '', inplace = True)\n",
    "fullDF['Brand_name'] = fullDF['Brand_name'].apply(lambda x: x.lower().replace(\"'\", \"\").replace(\",\", \"\").replace(\":\", \"\").replace(\"-\", \"\").replace(\".\", \"\"))\n",
    "fullDF['Name'] = fullDF['Name'].apply(lambda x: x.lower().replace(\"'\", \"\").replace(\",\", \"\").replace(\":\", \"\").replace(\"-\", \"\").replace(\".\", \"\"))\n",
    "\n",
    "# combine brand and name fields\n",
    "fullDF['brandAlready'] = fullDF.apply(lambda x: x['Name'].find(x['Brand_name']), axis = 1)\n",
    "fullDF.loc[fullDF.brandAlready == -1, 'Name'] = fullDF.loc[fullDF.brandAlready == -1, 'Brand_name'] + \\\n",
    "    ' ' + fullDF.loc[fullDF.brandAlready == -1, 'Name']\n",
    "fullDF.drop('brandAlready', axis = 1, inplace = True)\n",
    "\n",
    "# split data\n",
    "trainDF = pd.DataFrame(fullDF.loc[fullDF['origin'] == 'train'].drop('origin', axis = 1))\n",
    "testDF = pd.DataFrame(fullDF.loc[fullDF['origin'] == 'test'].drop(['origin', 'Category'], axis = 1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find total number of unique words\n",
    "unique_words = np.unique(np.array(' '.join(np.array(fullDF['Name'])).split(' ')))\n",
    "max_length = len(unique_words)\n",
    "\n",
    "# initialize empty dataframe\n",
    "wordBag = pd.DataFrame(np.zeros((len(fullDF), max_length)), \n",
    "                       columns = unique_words)\n",
    "\n",
    "# loop through product names\n",
    "for i, productName in enumerate(fullDF['Name'].apply(lambda x: x.split(' '))):\n",
    "    # loop through words in name\n",
    "    for word in productName:\n",
    "        # identify word presence\n",
    "        wordBag.loc[i, word] = 1\n",
    "\n",
    "# split word bag\n",
    "trainDFwordBag = wordBag.loc[0:7999]\n",
    "testDFwordBag = wordBag.loc[8000:9999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras tokenizer function\n",
    "def textVecGen(size):\n",
    "    # train keras tokenizer\n",
    "    tokenizer = TextVectorization(max_tokens = size, \n",
    "                                  output_sequence_length = size)\n",
    "    tokenizer.adapt(fullDF['Name'])\n",
    "\n",
    "    # vectorize data\n",
    "    textVecDF = pd.DataFrame(tokenizer(fullDF['Name']))\n",
    "    \n",
    "    # split vectorized data\n",
    "    trainDFtextVec = textVecDF.loc[0:7999]\n",
    "    testDFtextVec = textVecDF.loc[8000:9999]\n",
    "\n",
    "    return(trainDFtextVec, testDFtextVec)\n",
    "\n",
    "# generate vectorized data\n",
    "trainDFtextVec1000, testDFtextVec1000 = textVecGen(1000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seeds\n",
    "np.random.seed(542023)\n",
    "tf.random.set_seed(542023)\n",
    "\n",
    "# define model architecture\n",
    "model1 = Sequential([\n",
    "    Dense(512, activation = 'relu'),\n",
    "    Dense(256, activation = 'relu'),\n",
    "    Dense(128, activation = 'relu'),\n",
    "    Dense(64, activation = 'relu'),\n",
    "    Dense(7, activation = 'softmax')\n",
    "])\n",
    "\n",
    "# print model summary\n",
    "model1.summary()\n",
    "\n",
    "# define F1 metric\n",
    "f1_score_metric = F1Score(num_classes = 7, average = 'weighted')\n",
    "\n",
    "# compile model\n",
    "model1.compile(optimizer = 'rmsprop',\n",
    "               loss = 'categorical_crossentropy',\n",
    "               metrics = ['accuracy', f1_score_metric])\n",
    "    \n",
    "# define early stopping criterion\n",
    "early = EarlyStopping(monitor = 'f1_score', mode = 'max', patience = 3)\n",
    "\n",
    "# train deep learning model\n",
    "trained1 = model1.fit(trainDFwordBag,\n",
    "                      to_categorical(trainDF['Cat_code']),\n",
    "                      epochs = 100,\n",
    "                      batch_size = 128,\n",
    "                      callbacks = early,\n",
    "                      verbose = 1)\n",
    "\n",
    "# predict on test set\n",
    "pred1 = model1.predict(trainDFwordBag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seeds\n",
    "np.random.seed(542023)\n",
    "tf.random.set_seed(542023)\n",
    "\n",
    "# define model architecture\n",
    "model2 = Sequential([\n",
    "    Embedding(1000, 128, input_shape = (1000, )),\n",
    "    Flatten(),\n",
    "    Dense(128, activation = 'relu'),\n",
    "    Dense(64, activation = 'relu'),\n",
    "    Dense(7, activation = 'softmax')\n",
    "])\n",
    "\n",
    "# print model summary\n",
    "model2.summary()\n",
    "\n",
    "# define F1 metric\n",
    "f1_score_metric = F1Score(num_classes = 7, average = 'weighted')\n",
    "\n",
    "# compile model\n",
    "model2.compile(optimizer = 'rmsprop',\n",
    "               loss = 'categorical_crossentropy',\n",
    "               metrics = ['accuracy', f1_score_metric])\n",
    "    \n",
    "# define early stopping criterion\n",
    "early = EarlyStopping(monitor = 'f1_score', mode = 'max', patience = 3)\n",
    "\n",
    "# train deep learning model\n",
    "trained2 = model2.fit(trainDFtextVec1000,\n",
    "                      to_categorical(trainDF['Cat_code']),\n",
    "                      epochs = 100,\n",
    "                      batch_size = 128,\n",
    "                      callbacks = early,\n",
    "                      verbose = 1)\n",
    "\n",
    "# predict on test set\n",
    "pred2 = model1.predict(trainDFtextVec1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsci",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
