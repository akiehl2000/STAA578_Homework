{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import analysis packages\n",
    "import gensim\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import keras\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Dense, Dropout, Embedding, Flatten, SimpleRNN, TextVectorization\n",
    "from keras.models import Sequential\n",
    "from keras.regularizers import l2\n",
    "from keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import tensorflow as tf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data from .csv files\n",
    "trainDF = pd.read_csv('./ibotta_train.csv')\n",
    "testDF = pd.read_csv('./ibotta_test.csv')\n",
    "\n",
    "# combine data sets for preprocessing\n",
    "trainDF['origin'] = 'train'\n",
    "testDF['origin'] = 'test'\n",
    "fullDF = pd.concat([trainDF, testDF])\n",
    "\n",
    "# text cleaning\n",
    "fullDF['Brand_name'].where(-fullDF['Brand_name'].isna(), '', inplace = True)\n",
    "fullDF['Brand_name'] = fullDF['Brand_name'].apply(lambda x: x.lower().replace(\"'\", \"\").replace(\",\", \"\").replace(\":\", \"\").replace(\"-\", \"\").replace(\".\", \"\"))\n",
    "fullDF['Name'] = fullDF['Name'].apply(lambda x: x.lower().replace(\"'\", \"\").replace(\",\", \"\").replace(\":\", \"\").replace(\"-\", \"\").replace(\".\", \"\"))\n",
    "\n",
    "# combine brand and name fields\n",
    "fullDF['brandAlready'] = fullDF.apply(lambda x: x['Name'].find(x['Brand_name']), axis = 1)\n",
    "fullDF.loc[fullDF.brandAlready == -1, 'Name'] = fullDF.loc[fullDF.brandAlready == -1, 'Brand_name'] + \\\n",
    "    ' ' + fullDF.loc[fullDF.brandAlready == -1, 'Name']\n",
    "fullDF.drop('brandAlready', axis = 1, inplace = True)\n",
    "\n",
    "# seed random seed\n",
    "random.seed(542023)\n",
    "\n",
    "# split data\n",
    "trainDF = pd.DataFrame(fullDF.loc[fullDF['origin'] == 'train'].drop('origin', axis = 1))\n",
    "validIdx = random.sample(list(trainDF['Id'] - 1), 1000)\n",
    "validDF = trainDF.loc[validIdx]\n",
    "trainDF = trainDF.loc[trainDF['Id'].apply(lambda x: x not in validIdx)]\n",
    "testDF = pd.DataFrame(fullDF.loc[fullDF['origin'] == 'test'].drop(['origin', 'Category'], axis = 1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find total number of unique words\n",
    "unique_words = np.unique(np.array(' '.join(np.array(fullDF['Name'])).split(' ')))\n",
    "max_length = len(unique_words)\n",
    "\n",
    "# initialize empty dataframe\n",
    "wordBag = pd.DataFrame(np.zeros((len(fullDF), max_length)), \n",
    "                       columns = unique_words)\n",
    "\n",
    "# loop through product names\n",
    "for i, productName in enumerate(fullDF['Name'].apply(lambda x: x.split(' '))):\n",
    "    # loop through words in name\n",
    "    for word in productName:\n",
    "        # identify word presence\n",
    "        wordBag.loc[i, word] = 1\n",
    "\n",
    "# split word bag\n",
    "trainDFwordBag = wordBag.loc[0:7999]\n",
    "validDFwordBag = trainDFwordBag.loc[validIdx]\n",
    "trainDFwordBag = trainDFwordBag.loc[pd.Series(trainDFwordBag.index).apply(lambda x: x not in validIdx)]\n",
    "testDFwordBag = wordBag.loc[8000:9999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9999, 4977)\n",
      "(9999, 1000)\n",
      "(9999, 500)\n",
      "(9999, 100)\n"
     ]
    }
   ],
   "source": [
    "# tag data with iterable object\n",
    "tagged_data = [TaggedDocument(d, [i]) for i, d in enumerate(fullDF['Name'].apply(lambda x: x.split(' ')))]\n",
    "\n",
    "# doc2vec vectorization function\n",
    "def doc2vecGen(size):\n",
    "    # train doc2vec tokenizer\n",
    "    tokenizer = Doc2Vec(tagged_data, vector_size = size, min_count = 1, epochs = 100)\n",
    "    tokenizer.build_vocab(list(tagged_data))\n",
    "    tokenizer.train(list(tagged_data), \n",
    "                    total_examples = tokenizer.corpus_count, \n",
    "                    epochs = tokenizer.epochs)\n",
    "    \n",
    "    # vectorize text data\n",
    "    doc2vecDF = fullDF['Name'].apply(lambda x: tokenizer.infer_vector(x.split(' ')))\n",
    "    doc2vecDF = pd.DataFrame(np.array(doc2vecDF))[0].apply(pd.Series)\n",
    "\n",
    "    # split vectorized data\n",
    "    trainDFdoc2vec = doc2vecDF.loc[0:7999]\n",
    "    validDFdoc2vec = doc2vecDF.loc[validIdx]\n",
    "    trainDFdoc2vec = trainDFdoc2vec.loc[pd.Series(trainDFdoc2vec.index).apply(lambda x: x not in validIdx)]\n",
    "    testDFdoc2vec = doc2vecDF.loc[8000:9999]\n",
    "\n",
    "    return(trainDFdoc2vec, validDFdoc2vec, testDFdoc2vec)\n",
    "\n",
    "# generate vectorized data\n",
    "# trainDFdoc2vecMAX, validDFdoc2vecMAX, testDFdoc2vecMAX = doc2vecGen(max_length)\n",
    "# trainDFdoc2vec1000, validDFdoc2vec1000, testDFdoc2vec1000 = doc2vecGen(1000)\n",
    "# trainDFdoc2vec500, validDFdoc2vec500, testDFdoc2vec500 = doc2vecGen(500)\n",
    "# trainDFdoc2vec100, validDFdoc2vec100, testDFdoc2vec100 = doc2vecGen(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n",
      "\n",
      "systemMemory: 8.00 GB\n",
      "maxCacheSize: 2.67 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-05 13:06:08.340005: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    }
   ],
   "source": [
    "# keras tokenizer function\n",
    "def textVecGen(size):\n",
    "    # train keras tokenizer\n",
    "    tokenizer = TextVectorization(max_tokens = size,\n",
    "                                  output_sequence_length = size)\n",
    "    tokenizer.adapt(fullDF['Name'])\n",
    "\n",
    "    # vectorize data\n",
    "    textVecDF = pd.DataFrame(tokenizer(fullDF['Name']))\n",
    "    \n",
    "    # split vectorized data\n",
    "    trainDFtextVec = textVecDF.loc[0:7999]\n",
    "    validDFtextVec = trainDFtextVec.loc[validIdx]\n",
    "    trainDFtextVec = trainDFtextVec.loc[pd.Series(trainDFtextVec.index).apply(lambda x: x not in validIdx)]\n",
    "    testDFtextVec = textVecDF.loc[8000:9999]\n",
    "\n",
    "    return(trainDFtextVec, validDFtextVec, testDFtextVec)\n",
    "\n",
    "# generate vectorized data\n",
    "trainDFtextVecMAX, validDFtextVecMAX, testDFtextVecMAX = textVecGen(max_length)\n",
    "trainDFtextVec1000, validDFtextVec1000, testDFtextVec1000 = textVecGen(1000)\n",
    "trainDFtextVec500, validDFtextVec500, testDFtextVec500 = textVecGen(500)\n",
    "trainDFtextVec100, validDFtextVec100, testDFtextVec100 = textVecGen(100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model evaluation function\n",
    "def EvaluateModel(model, X_train):\n",
    "    # set random seeds\n",
    "    np.random.seed(542023)\n",
    "    tf.random.set_seed(542023)\n",
    "\n",
    "    # print model summary\n",
    "    try:\n",
    "        model.summary()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # compile model\n",
    "    model.compile(optimizer = 'rmsprop',\n",
    "                  loss = 'categorical_crossentropy',\n",
    "                  metrics = ['accuracy'])\n",
    "    \n",
    "    # define early stopping criterion\n",
    "    early = EarlyStopping(monitor = 'val_loss', mode = 'min', patience = 3)\n",
    "\n",
    "    # train deep learning model\n",
    "    trained = model.fit(X_train,\n",
    "                        to_categorical(trainDF['Cat_code']),\n",
    "                        epochs = 100,\n",
    "                        batch_size = 128,\n",
    "                        callbacks = early,\n",
    "                        validation_split = 0.15,\n",
    "                        verbose = 1)\n",
    "    \n",
    "    # prepare model evaluation\n",
    "    acc = trained.history['accuracy']\n",
    "    val_acc = trained.history['val_accuracy']\n",
    "    loss = trained.history['loss']\n",
    "    val_loss = trained.history['val_loss']\n",
    "    epochs = range(1, len(acc) + 1)\n",
    "\n",
    "    # final validation accuracy\n",
    "    display(f\"Internal validation accuracy: {round(val_acc[-1] * 100, 2)}%\")\n",
    "\n",
    "    # plot training accuracy\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize = (15, 5))\n",
    "    ax1.plot(epochs, acc, 'bo', label = 'Training acc')\n",
    "    ax1.plot(epochs, val_acc, 'b', label = 'Validation acc')\n",
    "    ax1.set(xlabel = 'Epochs', ylabel = 'Accuracy')\n",
    "    ax1.legend()\n",
    "    ax2.plot(epochs, loss, 'bo', label = 'Training loss')\n",
    "    ax2.plot(epochs, val_loss, 'b', label = 'Validation loss')\n",
    "    ax2.set(xlabel = 'Epochs', ylabel = 'Loss')\n",
    "    ax2.legend()\n",
    "    fig.suptitle('Evaluation Metrics')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, None, 64)          6400      \n",
      "                                                                 \n",
      " simple_rnn_6 (SimpleRNN)    (None, 32)                3104      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               4224      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 7)                 455       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 22,439\n",
      "Trainable params: 22,439\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      " 1/47 [..............................] - ETA: 3:44 - loss: 1.9366 - accuracy: 0.2422"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 11\u001b[0m\n\u001b[1;32m      2\u001b[0m model \u001b[39m=\u001b[39m Sequential([\n\u001b[1;32m      3\u001b[0m     Embedding(\u001b[39m100\u001b[39m, \u001b[39m64\u001b[39m),\n\u001b[1;32m      4\u001b[0m     SimpleRNN(\u001b[39m32\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      7\u001b[0m     Dense(\u001b[39m7\u001b[39m, activation \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39msoftmax\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      8\u001b[0m ])\n\u001b[1;32m     10\u001b[0m \u001b[39m# evaluate model\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m EvaluateModel(model, trainDFtextVec100)\n",
      "Cell \u001b[0;32mIn[6], line 22\u001b[0m, in \u001b[0;36mEvaluateModel\u001b[0;34m(model, X_train)\u001b[0m\n\u001b[1;32m     19\u001b[0m early \u001b[39m=\u001b[39m EarlyStopping(monitor \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mval_loss\u001b[39m\u001b[39m'\u001b[39m, mode \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mmin\u001b[39m\u001b[39m'\u001b[39m, patience \u001b[39m=\u001b[39m \u001b[39m3\u001b[39m)\n\u001b[1;32m     21\u001b[0m \u001b[39m# train deep learning model\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m trained \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(X_train,\n\u001b[1;32m     23\u001b[0m                     to_categorical(trainDF[\u001b[39m'\u001b[39;49m\u001b[39mCat_code\u001b[39;49m\u001b[39m'\u001b[39;49m]),\n\u001b[1;32m     24\u001b[0m                     epochs \u001b[39m=\u001b[39;49m \u001b[39m100\u001b[39;49m,\n\u001b[1;32m     25\u001b[0m                     batch_size \u001b[39m=\u001b[39;49m \u001b[39m128\u001b[39;49m,\n\u001b[1;32m     26\u001b[0m                     callbacks \u001b[39m=\u001b[39;49m early,\n\u001b[1;32m     27\u001b[0m                     validation_split \u001b[39m=\u001b[39;49m \u001b[39m0.15\u001b[39;49m,\n\u001b[1;32m     28\u001b[0m                     verbose \u001b[39m=\u001b[39;49m \u001b[39m1\u001b[39;49m)\n\u001b[1;32m     30\u001b[0m \u001b[39m# prepare model evaluation\u001b[39;00m\n\u001b[1;32m     31\u001b[0m acc \u001b[39m=\u001b[39m trained\u001b[39m.\u001b[39mhistory[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/dsci/lib/python3.8/site-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/dsci/lib/python3.8/site-packages/keras/engine/training.py:1409\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1402\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1403\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m   1404\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   1405\u001b[0m     step_num\u001b[39m=\u001b[39mstep,\n\u001b[1;32m   1406\u001b[0m     batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[1;32m   1407\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m   1408\u001b[0m   callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1409\u001b[0m   tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   1410\u001b[0m   \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1411\u001b[0m     context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/miniconda3/envs/dsci/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/dsci/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    912\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    914\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 915\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    917\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    918\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/miniconda3/envs/dsci/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    945\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    946\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 947\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_stateless_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    949\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    950\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    951\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/miniconda3/envs/dsci/lib/python3.8/site-packages/tensorflow/python/eager/function.py:2453\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2450\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m   2451\u001b[0m   (graph_function,\n\u001b[1;32m   2452\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2453\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m   2454\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[0;32m~/miniconda3/envs/dsci/lib/python3.8/site-packages/tensorflow/python/eager/function.py:1860\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1856\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1857\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1858\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1859\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1860\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[1;32m   1861\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[1;32m   1862\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1863\u001b[0m     args,\n\u001b[1;32m   1864\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1865\u001b[0m     executing_eagerly)\n\u001b[1;32m   1866\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/miniconda3/envs/dsci/lib/python3.8/site-packages/tensorflow/python/eager/function.py:497\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[1;32m    496\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 497\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m    498\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[1;32m    499\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[1;32m    500\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m    501\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m    502\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[1;32m    503\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    504\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    505\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[1;32m    506\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    509\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[1;32m    510\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/miniconda3/envs/dsci/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# define model architecture\n",
    "model = Sequential([\n",
    "    Embedding(100, 64),\n",
    "    SimpleRNN(32),\n",
    "    Dense(128, activation = 'relu'),\n",
    "    Dense(64, activation = 'relu'),\n",
    "    Dense(7, activation = 'softmax')\n",
    "])\n",
    "\n",
    "# evaluate model\n",
    "EvaluateModel(model, trainDFtextVec100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model architecture\n",
    "model = Sequential([\n",
    "    Embedding(100, 8),\n",
    "    SimpleRNN(32, return_sequences = True),\n",
    "    SimpleRNN(32, return_sequences = True),\n",
    "    SimpleRNN(32),\n",
    "    Dense(7, activation = 'softmax')\n",
    "])\n",
    "\n",
    "# evaluate model\n",
    "EvaluateModel(model, trainDFtextVec100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model architecture\n",
    "model = Sequential([\n",
    "    Embedding(1000, 128),\n",
    "    SimpleRNN(32),\n",
    "    Dense(128, activation = 'relu'),\n",
    "    Dense(64, activation = 'relu'),\n",
    "    Dense(7, activation = 'softmax')\n",
    "])\n",
    "\n",
    "# evaluate model\n",
    "EvaluateModel(model, trainDFtextVec1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model architecture\n",
    "model = Sequential([\n",
    "    Embedding(100, 128, input_shape = (100, )),\n",
    "    SimpleRNN(32),\n",
    "    Dense(128, activation = 'relu'),\n",
    "    Dense(64, activation = 'relu'),\n",
    "    Dense(7, activation = 'softmax')\n",
    "])\n",
    "\n",
    "# evaluate model\n",
    "EvaluateModel(model, trainDFdoc2vec100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model architecture\n",
    "model = Sequential([\n",
    "    Embedding(1000, 128, input_shape = (1000, )),\n",
    "    SimpleRNN(32),\n",
    "    Dense(128, activation = 'relu'),\n",
    "    Dense(64, activation = 'relu'),\n",
    "    Dense(7, activation = 'softmax')\n",
    "])\n",
    "\n",
    "# evaluate model\n",
    "EvaluateModel(model, trainDFdoc2vec1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsci",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
